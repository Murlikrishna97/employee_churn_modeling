{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RowNumber  CustomerId   Surname  ...  IsActiveMember EstimatedSalary Exited\n",
      "0          1    15634602  Hargrave  ...               1       101348.88      1\n",
      "1          2    15647311      Hill  ...               1       112542.58      0\n",
      "2          3    15619304      Onio  ...               0       113931.57      1\n",
      "3          4    15701354      Boni  ...               0        93826.63      0\n",
      "4          5    15737888  Mitchell  ...               1        79084.10      0\n",
      "\n",
      "[5 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Churn_Modelling.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore Geography  Gender  ...  IsActiveMember  EstimatedSalary  Exited\n",
       "0             619    France  Female  ...               1        101348.88       1\n",
       "1             608     Spain  Female  ...               1        112542.58       0\n",
       "2             502    France  Female  ...               0        113931.57       1\n",
       "3             699    France  Female  ...               0         93826.63       0\n",
       "4             850     Spain  Female  ...               1         79084.10       0\n",
       "...           ...       ...     ...  ...             ...              ...     ...\n",
       "9995          771    France    Male  ...               0         96270.64       0\n",
       "9996          516    France    Male  ...               1        101699.77       0\n",
       "9997          709    France  Female  ...               1         42085.58       1\n",
       "9998          772   Germany    Male  ...               0         92888.52       1\n",
       "9999          792    France  Female  ...               0         38190.78       0\n",
       "\n",
       "[10000 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the columns that are not needed\n",
    "\n",
    "df = df.drop([\"RowNumber\", \"CustomerId\", \"Surname\"], axis=1)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore Geography  Gender  ...  IsActiveMember  EstimatedSalary  Exited\n",
       "0             619    France       0  ...               1        101348.88       1\n",
       "1             608     Spain       0  ...               1        112542.58       0\n",
       "2             502    France       0  ...               0        113931.57       1\n",
       "3             699    France       0  ...               0         93826.63       0\n",
       "4             850     Spain       0  ...               1         79084.10       0\n",
       "...           ...       ...     ...  ...             ...              ...     ...\n",
       "9995          771    France       1  ...               0         96270.64       0\n",
       "9996          516    France       1  ...               1        101699.77       0\n",
       "9997          709    France       0  ...               1         42085.58       1\n",
       "9998          772   Germany       1  ...               0         92888.52       1\n",
       "9999          792    France       0  ...               0         38190.78       0\n",
       "\n",
       "[10000 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Convert categorical variables to numerical\n",
    "\n",
    "# LabelEncoder converts categorical text data into numerical values (0, 1, 2, etc.)\n",
    "label_encoder_gender = LabelEncoder()\n",
    "df['Gender'] = label_encoder_gender.fit_transform(df['Gender'])\n",
    "df\n",
    "# What does fit_transform do?\n",
    "# fit: The encoder learns the possible categories in the 'Gender' column (in this case, 'Male' and 'Female').\n",
    "# transform: It converts those categories to numerical values (like 'Female' to 0 and 'Male' to 1).\n",
    "\n",
    "# What's happening behind the scenes:\n",
    "\n",
    "# The encoder sees all unique values in the 'Gender' column\n",
    "# It assigns a unique integer to each value (alphabetically by default)\n",
    "# It replaces each original value with its corresponding integer\n",
    "# The mapping is stored in the encoder for future use (like when processing new data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 10000 stored elements and shape (10000, 3)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OneHotEncode the Geography column\n",
    "\n",
    "# OneHotEncoder creates binary columns for each category in a categorical feature. Each column represents one possible category and uses 1 or 0 to indicate if that category is present.\n",
    "\n",
    "# Original data: ['France', 'Spain', 'Germany']\n",
    "# After OneHotEncoder:\n",
    "# France_column: [1, 0, 0]\n",
    "# Spain_column:  [0, 1, 0]\n",
    "# Germany_column:[0, 0, 1]\n",
    "\n",
    "onehot_encoder_geography = OneHotEncoder()\n",
    "geography_encoder = onehot_encoder_geography.fit_transform(df[['Geography']])\n",
    "geography_encoder\n",
    "\n",
    "\n",
    "# For Geography (OneHotEncoder):\n",
    "# Geography has three categories: France, Germany, and Spain\n",
    "# If we used LabelEncoder, we'd get: France=0, Germany=1, Spain=2\n",
    "# This would be a problem because:\n",
    "# The model might think Germany (1) is \"between\" France (0) and Spain (2)\n",
    "# The model might think Spain (2) is \"twice as far\" from France (0) as Germany (1)\n",
    "# These numerical relationships don't make sense for country names\n",
    "\n",
    "# Simple explanation:\n",
    "# For Gender, we can use a simple 0/1 encoding because there are only two options\n",
    "# For Geography, we need to create separate yes/no columns for each country to avoid implying that countries have a numerical order or relationship to each other\n",
    "\n",
    "\n",
    "# A simple rule of thumb:\n",
    "\n",
    "# Use LabelEncoder when:\n",
    "\n",
    "# The category is the target variable (what you're predicting)\n",
    "# The category has a natural order (like Small, Medium, Large)\n",
    "# There are only two categories (binary) and you're comfortable with them being 0 and 1\n",
    "\n",
    "# Use OneHotEncoder when:\n",
    "\n",
    "# The category has no natural order\n",
    "# There are more than two categories\n",
    "# The category is an input feature (not the target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Geography_France', 'Geography_Germany', 'Geography_Spain'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_encoder_geography.get_feature_names_out([\"Geography\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Geography_France  Geography_Germany  Geography_Spain\n",
       "0                  1.0                0.0              0.0\n",
       "1                  0.0                0.0              1.0\n",
       "2                  1.0                0.0              0.0\n",
       "3                  1.0                0.0              0.0\n",
       "4                  0.0                0.0              1.0\n",
       "...                ...                ...              ...\n",
       "9995               1.0                0.0              0.0\n",
       "9996               1.0                0.0              0.0\n",
       "9997               1.0                0.0              0.0\n",
       "9998               0.0                1.0              0.0\n",
       "9999               1.0                0.0              0.0\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_encoder_df = pd.DataFrame(geography_encoder.toarray(),columns=onehot_encoder_geography.get_feature_names_out([\"Geography\"]))\n",
    "geo_encoder_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore  Gender  ...  Geography_Germany  Geography_Spain\n",
       "0          619       0  ...                0.0              0.0\n",
       "1          608       0  ...                0.0              1.0\n",
       "2          502       0  ...                0.0              0.0\n",
       "3          699       0  ...                0.0              0.0\n",
       "4          850       0  ...                0.0              1.0\n",
       "\n",
       "[5 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combine the onehot encoded geography with the original dataframe\n",
    "\n",
    "df = pd.concat([df.drop(\"Geography\",axis=1),geo_encoder_df],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the encoders and scaler\n",
    "\n",
    "with open('label_encoder_gender.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder_gender, f)\n",
    "\n",
    "with open('onehot_encoder_geography.pkl', 'wb') as f:\n",
    "    pickle.dump(onehot_encoder_geography, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the dataset into training and testing sets\n",
    "\n",
    "X = df.drop([\"Exited\"],axis=1)\n",
    "y = df[\"Exited\"]\n",
    "\n",
    "# The random_state parameter is a seed value for the random number generator\n",
    "# Setting it to a fixed value (like 42) ensures that the train-test split is reproducible\n",
    "# This means running the same code multiple times will produce identical splits\n",
    "# Without setting random_state, each run would produce different train/test sets\n",
    "# This reproducibility is crucial for:\n",
    "#   1. Debugging and troubleshooting model issues\n",
    "#   2. Comparing different models on the same data splits\n",
    "#   3. Ensuring scientific reproducibility of results\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# StandardScaler standardizes numerical features by removing the mean and scaling to unit variance. This transforms your data so that it has a mean of 0 and a standard deviation of 1.\n",
    "# Used when numerical features are in different scales like age in years vs. salary in thousands,\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# StandardScaler makes all your numerical features have similar scales by:\n",
    "# Subtracting the average (so values center around 0)\n",
    "# Dividing by the standard deviation (so most values fall between -1 and 1)\n",
    "\n",
    "# Is it necessary for all ML models?\n",
    "# Not always necessary, but often helpful for:\n",
    "# Neural Networks (like in your code)\n",
    "# Support Vector Machines\n",
    "# K-means clustering\n",
    "\n",
    "# Any algorithm that uses distances or gradients\n",
    "# Less important for:\n",
    "# Decision Trees\n",
    "# Random Forests\n",
    "# Some other tree-based methods\n",
    "\n",
    "# Why it helps neural networks specifically:\n",
    "# Faster learning: Neural networks learn faster when features have similar scales\n",
    "# More stable: Prevents some weights from becoming extremely large while others stay tiny\n",
    "# Better accuracy: Often leads to better predictions\n",
    "\n",
    "# Real-world example:\n",
    "# Imagine trying to predict house prices using:\n",
    "# Number of rooms (typically 1-10)\n",
    "# Square footage (typically 500-5000)\n",
    "# Price (typically $100,000-$1,000,000)\n",
    "# Without scaling, the model might focus too much on price (because the numbers are bigger) and ignore the number of rooms (because the numbers are smaller), even though both features might be equally important for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.57749609,  0.91324755, -0.6557859 , ..., -0.99850112,\n",
       "         1.72572313, -0.57638802],\n",
       "       [-0.29729735,  0.91324755,  0.3900109 , ...,  1.00150113,\n",
       "        -0.57946723, -0.57638802],\n",
       "       [-0.52560743, -1.09499335,  0.48508334, ..., -0.99850112,\n",
       "        -0.57946723,  1.73494238],\n",
       "       ...,\n",
       "       [ 0.81311987, -1.09499335,  0.77030065, ...,  1.00150113,\n",
       "        -0.57946723, -0.57638802],\n",
       "       [ 0.41876609,  0.91324755, -0.94100321, ...,  1.00150113,\n",
       "        -0.57946723, -0.57638802],\n",
       "       [-0.24540869,  0.91324755,  0.00972116, ..., -0.99850112,\n",
       "         1.72572313, -0.57638802]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>771</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>516</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>709</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>772</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>792</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore  Gender  ...  Geography_Germany  Geography_Spain\n",
       "0             619       0  ...                0.0              0.0\n",
       "1             608       0  ...                0.0              1.0\n",
       "2             502       0  ...                0.0              0.0\n",
       "3             699       0  ...                0.0              0.0\n",
       "4             850       0  ...                0.0              1.0\n",
       "...           ...     ...  ...                ...              ...\n",
       "9995          771       1  ...                0.0              0.0\n",
       "9996          516       1  ...                0.0              0.0\n",
       "9997          709       0  ...                0.0              0.0\n",
       "9998          772       1  ...                1.0              0.0\n",
       "9999          792       0  ...                0.0              0.0\n",
       "\n",
       "[10000 rows x 13 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN Implementation\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 12)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build our ANN model\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64,activation=\"relu\",input_shape=(X_train.shape[1],)), # Hidden Layer 1\n",
    "    Dense(32,activation=\"relu\"), # Hidden Layer 2\n",
    "    Dense(1,activation=\"sigmoid\") # Output Layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Think of the below code as designing the \"brain\" of your prediction model.\n",
    "\n",
    "# model = Sequential([...]):\n",
    "# What it is: This creates a basic type of neural network where information flows straight through, layer by layer, like a production line. Sequential means the layers are stacked one after the other.\n",
    "# Analogy: Imagine building with LEGOs; you stack one brick on top of the next.\n",
    "\n",
    "# Dense(...):\n",
    "# What it is: This defines a \"layer\" in your network. \"Dense\" means every \"neuron\" (or processing unit) in this layer is connected to every neuron in the previous layer. It's the most common type of layer.\n",
    "# Analogy: Think of it as a group of workers. In a dense layer, every worker in this group talks to every worker in the previous group to get information.\n",
    "\n",
    "# Dense(64, activation=\"relu\", input_shape=(X_train.shape[1],)) - Hidden Layer 1:\n",
    "# 64 (Number of Neurons): This is the number of processing units or \"neurons\" in this first hidden layer.\n",
    "# Why 64? This number is a hyperparameter – a setting you choose before training. There's no magic formula! Common practices include:\n",
    "# Starting with powers of 2 (like 32, 64, 128).\n",
    "# Choosing a number related to the input features (you have X_train.shape[1] features, which was 12 in your notebook). 64 is larger than 12, allowing the network to potentially learn complex combinations of those features.\n",
    "# Experimentation: Often, developers try different numbers (e.g., 32, 64, 128) and see which performs best on their specific problem. 64 is a reasonable starting point.\n",
    "\n",
    "# activation=\"relu\" (Activation Function): This is like an \"on/off\" switch or a filter for each neuron. It decides what information gets passed to the next layer.\n",
    "# What ReLU does: If the neuron's input is positive, it passes that value along. If it's zero or negative, it outputs zero. (Think: max(0, input)).\n",
    "# Why ReLU? It's very popular for hidden layers because:\n",
    "# It's simple and fast to compute.\n",
    "# It helps prevent a problem called the \"vanishing gradient,\" making training more effective, especially in deeper networks.\n",
    "#  input_shape=(X_train.shape[1],): This tells the first layer what the input data looks like. X_train.shape[1] is the number of columns (features) in your training data (which was 12 after preprocessing). You only need to specify this for the very first layer.\n",
    "\n",
    "# Dense(32, activation=\"relu\") - Hidden Layer 2:\n",
    "# 32 (Number of Neurons): This second hidden layer has 32 neurons.\n",
    "# Why 32? Again, it's a hyperparameter. It's common to gradually decrease the number of neurons in deeper hidden layers. The idea is that the network combines features in the first layer and then refines those combinations into more abstract patterns in subsequent layers, requiring fewer neurons. Going from 64 to 32 follows this pattern.\n",
    "# activation=\"relu\": Same reason as before – a good default choice for hidden layers.\n",
    "\n",
    "# Dense(1, activation=\"sigmoid\") - Output Layer:\n",
    "# 1 (Number of Neurons): This final layer has only one neuron.\n",
    "# Why 1? Because you are doing binary classification. You want to predict one of two outcomes: either the customer Exited (1) or Did Not Exit (0). A single neuron is sufficient to output a single value representing this prediction.\n",
    "# activation=\"sigmoid\": This activation function is crucial for the output layer in binary classification.\n",
    "# What Sigmoid does: It squishes any input value into a range between 0 and 1.\n",
    "# Why Sigmoid? The output (between 0 and 1) can be interpreted as a probability. For example, an output of 0.85 means the model is 85% confident that the customer will exit. An output of 0.1 means it's only 10% confident they will exit. This is exactly what we need for predicting yes/no outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers Similar to Dense\n",
    "# Convolutional Layers (Conv1D, Conv2D, Conv3D)\n",
    "# Specialized for grid-like data (images, time series)\n",
    "# Detect patterns regardless of where they appear in the input\n",
    "# Example: Conv2D(32, kernel_size=(3,3)) for image processing\n",
    "\n",
    "# Recurrent Layers (SimpleRNN, LSTM, GRU)\n",
    "# Process sequential data where order matters\n",
    "# Maintain internal memory of previous inputs\n",
    "# Example: LSTM(64) for text or time series analysis\n",
    "\n",
    "# Embedding Layers\n",
    "# Convert categorical data (like words) into dense vectors\n",
    "# Learn meaningful representations of categories\n",
    "# Example: Embedding(vocab_size, embedding_dim) for text processing\n",
    "\n",
    "# Attention Layers\n",
    "# Help models focus on relevant parts of the input\n",
    "# Critical for modern language models\n",
    "# Example: MultiHeadAttention(num_heads=8) for transformer-based models\n",
    "\n",
    "\n",
    "# Hidden Layer Size and Count\n",
    "\n",
    "# Layer Size Progression\n",
    "# The \"half of previous\" rule (64→32) is a common starting point but not a strict requirement\n",
    "\n",
    "# Other common patterns:\n",
    "# Same size for all layers (64→64→64)\n",
    "# Gradual reduction (128→64→32)\n",
    "# Bottleneck architecture (64→32→64) for autoencoders\n",
    "\n",
    "# Deciding Number of Hidden Layers\n",
    "\n",
    "# There's no one-size-fits-all rule. Consider:\n",
    "# 1. Problem Complexity:\n",
    "# Simple problems: 1-2 layers often sufficient\n",
    "# Complex problems: May benefit from 3+ layers\n",
    "\n",
    "# 2. Data Amount:\n",
    "# More data can support deeper networks\n",
    "# Limited data may overfit with too many layers\n",
    "\n",
    "# 3. Computational Resources:\n",
    "# Deeper networks require more computation\n",
    "# Consider your hardware limitations\n",
    "\n",
    "# 4. Empirical Testing:\n",
    "# Start simple (1-2 layers)\n",
    "# Add layers if underfitting (poor performance on both training and validation)\n",
    "# Remove layers if overfitting (good on training, poor on validation)\n",
    "# Monitor validation performance to guide decisions\n",
    "\n",
    "\n",
    "# Understanding Sigmoid vs. Softmax Activation Functions\n",
    "\n",
    "## Sigmoid Activation\n",
    "\n",
    "# **What it does:**\n",
    "# - Takes any input value and squeezes it into a range between 0 and 1\n",
    "# - Formula: `f(x) = 1 / (1 + e^(-x))`\n",
    "# - Outputs a single value that can be interpreted as a probability\n",
    "\n",
    "# **When to use:**\n",
    "# - **Binary classification problems** (predicting one of two classes)\n",
    "# - In the output layer when you need a yes/no prediction\n",
    "# - Examples: spam detection, customer churn prediction, disease diagnosis\n",
    "\n",
    "# **Why it works for binary tasks:**\n",
    "# - The output (between 0 and 1) represents the probability of the positive class\n",
    "# - Values closer to 1 indicate higher confidence in the positive class\n",
    "# - Values closer to 0 indicate higher confidence in the negative class\n",
    "# - Decision threshold is typically 0.5 (above = positive class, below = negative class)\n",
    "\n",
    "# ## Softmax Activation\n",
    "\n",
    "# **What it does:**\n",
    "# - Takes a vector of values and transforms them into a probability distribution\n",
    "# - All outputs sum to exactly 1.0\n",
    "# - Formula: `softmax(x)_i = e^(x_i) / Σ(e^(x_j))` for all j\n",
    "# - Emphasizes the largest values while suppressing lower values\n",
    "\n",
    "# **When to use:**\n",
    "# - **Multi-class classification problems** (predicting one of 3+ classes)\n",
    "# - In the output layer when you need to choose among several options\n",
    "# - Examples: image classification, language identification, product categorization\n",
    "\n",
    "# **Why it works for multi-class tasks:**\n",
    "# - Each output represents the probability for one specific class\n",
    "# - The class with the highest probability is the model's prediction\n",
    "# - Allows the model to express uncertainty across multiple classes\n",
    "# - Enables meaningful comparisons between different class probabilities\n",
    "\n",
    "# ## Key Differences\n",
    "\n",
    "# - **Sigmoid**: One output (0-1) for binary decisions\n",
    "# - **Softmax**: Multiple outputs (all sum to 1) for choosing among multiple options\n",
    "\n",
    "# Think of sigmoid as answering \"How likely is this single outcome?\" while softmax answers \"Which of these multiple outcomes is most likely?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                832       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2945 (11.50 KB)\n",
      "Trainable params: 2945 (11.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Understanding `model.summary()`\n",
    "\n",
    "# This summary gives you a quick overview of your model's architecture:\n",
    "\n",
    "# 1.  **`Model: \"sequential\"`**: Confirms you built a `Sequential` model.\n",
    "# 2.  **`Layer (type)`**: Lists each layer you added (`dense`, `dense_1`, `dense_2` are just default names) and their type (`Dense`).\n",
    "# 3.  **`Output Shape`**: Shows the shape of the data *coming out* of each layer.\n",
    "#     *   `(None, 64)`: The first layer outputs a batch of data (the `None` part means the batch size can vary) where each item has 64 values (one from each neuron).\n",
    "#     *   `(None, 32)`: The second layer outputs data with 32 values per item.\n",
    "#     *   `(None, 1)`: The final layer outputs data with 1 value per item (our probability).\n",
    "# 4.  **`Param #` (Parameters):** This is the most important part – it shows the number of *learnable values* (weights and biases) in each layer. These are the numbers the model adjusts during training to make better predictions.\n",
    "#     *   **`dense` (Layer 1):** 832 parameters.\n",
    "#         *   *How it's calculated:* You have 12 input features (`X_train.shape[1]`). Each input connects to each of the 64 neurons (12 \\* 64 = 768 weights). Each neuron also has 1 bias value (+ 64 biases). Total = 768 + 64 = 832.\n",
    "#     *   **`dense_1` (Layer 2):** 2080 parameters.\n",
    "#         *   *How it's calculated:* This layer takes input from the 64 neurons of the previous layer. Each of these 64 connects to the 32 neurons here (64 \\* 32 = 2048 weights). Each of the 32 neurons has 1 bias (+ 32 biases). Total = 2048 + 32 = 2080.\n",
    "#     *   **`dense_2` (Layer 3 - Output):** 33 parameters.\n",
    "#         *   *How it's calculated:* Takes input from the 32 neurons of the previous layer. Each connects to the 1 output neuron (32 \\* 1 = 32 weights). The output neuron has 1 bias (+ 1 bias). Total = 32 + 1 = 33.\n",
    "# 5.  **`Total params: 2945`**: The total number of values the model needs to learn during training.\n",
    "# 6.  **`Trainable params: 2945`**: The number of parameters that will actually be updated during training. Usually, this is the same as the total params unless you specifically \"freeze\" some layers.\n",
    "# 7.  **`Non-trainable params: 0`**: Parameters that are not updated during training (none in this case).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Understanding Neural Network Parameters: Weights and Biases\n",
    "\n",
    "# Let me explain these fundamental concepts in simple terms:\n",
    "\n",
    "## What Are Parameters?\n",
    "\n",
    "# **Parameters** are the values that a neural network learns during training. They're like the \"knobs\" the model adjusts to get better at making predictions. There are two types of parameters:\n",
    "\n",
    "# 1. **Weights**\n",
    "# 2. **Biases**\n",
    "\n",
    "# ## Weights Explained\n",
    "\n",
    "# **Weights** determine how strongly each input affects a neuron's output.\n",
    "\n",
    "# **Simple analogy:** Imagine you're deciding whether to go to the beach. Different factors influence your decision:\n",
    "# - Temperature (very important) → high weight\n",
    "# - Wind speed (somewhat important) → medium weight\n",
    "# - Day of week (less important) → low weight\n",
    "\n",
    "# In a neural network:\n",
    "# - Each connection between neurons has a weight\n",
    "# - Higher weight = stronger connection\n",
    "# - Weights can be positive (encouraging) or negative (discouraging)\n",
    "# - The network learns which connections should be strong and which should be weak\n",
    "\n",
    "# ## Biases Explained\n",
    "\n",
    "# **Biases** are additional values that help the model make better predictions even when all inputs are zero or very small.\n",
    "\n",
    "# **Simple analogy:** Continuing with the beach example:\n",
    "# - Even if all conditions (temperature, wind, etc.) are neutral, you might still have a personal preference\n",
    "# - If you generally like beaches, you have a positive bias toward going\n",
    "# - If you generally dislike beaches, you have a negative bias against going\n",
    "\n",
    "# In a neural network:\n",
    "# - Each neuron has its own bias\n",
    "# - Bias allows neurons to activate (or not) even when inputs are minimal\n",
    "# - It's like each neuron's \"default opinion\" before considering any inputs\n",
    "\n",
    "# ## Why One Bias Per Neuron?\n",
    "\n",
    "# Each neuron gets exactly one bias because:\n",
    "\n",
    "# 1. **Purpose of bias:** The bias shifts the activation function left or right. One value is sufficient to create this shift.\n",
    "\n",
    "# 2. **Mathematical necessity:** In the neuron's equation: `output = activation_function(sum(inputs × weights) + bias)`, one bias term is all that's needed.\n",
    "\n",
    "# 3. **Unique threshold:** Each neuron needs its own unique \"firing threshold\" - the bias provides this.\n",
    "\n",
    "\n",
    "# ## What Happens During Training\n",
    "\n",
    "# 1. The network starts with random weights and biases\n",
    "# 2. For each training example:\n",
    "#    - The network makes a prediction\n",
    "#    - The error is calculated (how far off the prediction was)\n",
    "#    - Weights and biases are adjusted slightly to reduce the error\n",
    "# 3. This process repeats thousands of times\n",
    "# 4. Gradually, the weights and biases evolve to capture patterns in your data\n",
    "\n",
    "# The beauty of neural networks is that they automatically learn which features are important (by assigning higher weights) and how they should be combined to make accurate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "\n",
    "optimizer = tensorflow.keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss = tensorflow.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Understanding Optimizers and Loss Functions in Neural Networks\n",
    "\n",
    "## The Optimizer (Adam)\n",
    "\n",
    "# ```python\n",
    "# optimizer = tensorflow.keras.optimizers.Adam(learning_rate=0.01)\n",
    "# ```\n",
    "\n",
    "# **What it does:** The optimizer controls *how* the model learns from its mistakes. It's like the \"learning strategy\" of your neural network.\n",
    "\n",
    "# **Adam specifically:**\n",
    "# - A popular and efficient optimizer that adapts the learning rate for each parameter\n",
    "# - Combines the benefits of two other optimizers (AdaGrad and RMSProp)\n",
    "# - Works well for most problems without much tuning\n",
    "\n",
    "# **Learning rate (0.01):**\n",
    "# - Controls how big of a step the model takes when updating weights\n",
    "# - Higher value (e.g., 0.1): Learns faster but might overshoot the optimal solution\n",
    "# - Lower value (e.g., 0.001): Learns more carefully but might take too long\n",
    "# - 0.01 is a common starting point that balances speed and stability\n",
    "\n",
    "# **Other popular optimizers:**\n",
    "# - **SGD** (Stochastic Gradient Descent): Simple but requires more tuning\n",
    "# - **RMSProp**: Good for recurrent neural networks\n",
    "# - **Adagrad**: Adapts learning rates based on parameter frequency\n",
    "# - **Adamax**: A variant of Adam that can be more stable in some cases\n",
    "\n",
    "# ## The Loss Function (Binary Cross-Entropy)\n",
    "\n",
    "# ```python\n",
    "# loss = tensorflow.keras.losses.BinaryCrossentropy()\n",
    "# ```\n",
    "\n",
    "# **What it does:** The loss function measures how wrong the model's predictions are. It's like the \"scoring system\" that tells the model how badly it's performing.\n",
    "\n",
    "# **Binary Cross-Entropy specifically:**\n",
    "# - Designed for binary classification problems (yes/no, 0/1)\n",
    "# - Measures the difference between predicted probabilities and actual values\n",
    "# - Heavily penalizes confident wrong predictions (e.g., predicting 0.9 when the true value is 0)\n",
    "# - Works well with sigmoid activation in the output layer\n",
    "\n",
    "# **When to use Binary Cross-Entropy:**\n",
    "# - When predicting one of two possible outcomes (like customer churn/no churn)\n",
    "# - When your output layer uses sigmoid activation\n",
    "# - When your target values are 0 or 1\n",
    "\n",
    "# **Other common loss functions:**\n",
    "# - **Categorical Cross-Entropy**: For multi-class classification (3+ classes)\n",
    "# - **Sparse Categorical Cross-Entropy**: Same as above but with integer labels\n",
    "# - **Mean Squared Error**: For regression problems (predicting continuous values)\n",
    "# - **Huber Loss**: For regression that's less sensitive to outliers\n",
    "\n",
    "# ## Compiling the Model\n",
    "\n",
    "# ```python\n",
    "# model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "# ```\n",
    "\n",
    "# This line puts everything together and prepares the model for training by specifying:\n",
    "\n",
    "# 1. **optimizer**: How the model should update its weights (Adam in this case)\n",
    "# 2. **loss**: How to measure prediction errors (Binary Cross-Entropy)\n",
    "# 3. **metrics**: What to track during training (\"accuracy\" measures the percentage of correct predictions)\n",
    "\n",
    "# You can specify the loss function either as a string (`\"binary_crossentropy\"`) or as an object (`loss=tensorflow.keras.losses.BinaryCrossentropy()`). Both approaches work the same way.\n",
    "\n",
    "# **The relationship between these components:**\n",
    "# - The model makes predictions\n",
    "# - The loss function calculates how wrong those predictions are\n",
    "# - The optimizer determines how to adjust the weights to reduce the loss\n",
    "# - The metrics track how well the model is performing\n",
    "\n",
    "# This process repeats for each batch of training data until the model learns to make accurate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Tensorboard\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorflow_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# TensorBoard is a visualization tool that helps you track and analyze your model's training progress.\n",
    "\n",
    "# Key features:\n",
    "# Creates interactive graphs of metrics (loss, accuracy) over time\n",
    "# Shows distributions of weights and biases\n",
    "# Helps identify problems like overfitting\n",
    "# Saves all data to the specified log directory with a timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True) \n",
    "\n",
    "\n",
    "# Understanding  EarlyStopping Callbacks\n",
    "\n",
    "## EarlyStopping\n",
    "\n",
    "# ```python\n",
    "# early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True)\n",
    "# ```\n",
    "\n",
    "# **What it does:** Automatically stops training when the model stops improving.\n",
    "\n",
    "# **Key parameters:**\n",
    "# - `monitor=\"val_loss\"`: Watches validation loss to determine improvement\n",
    "# - `patience=15`: Waits 15 epochs of no improvement before stopping\n",
    "# - `restore_best_weights=True`: Keeps the best model version, not the final one\n",
    "\n",
    "# **Why it's useful:** Prevents wasting time and computational resources on unproductive training and helps avoid overfitting by stopping before the model starts memorizing the training data.\n",
    "\n",
    "# Both callbacks are added to the `model.fit()` function to enhance the training process without changing the model itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3435 - accuracy: 0.8590 - val_loss: 0.3397 - val_accuracy: 0.8590\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3365 - accuracy: 0.8635 - val_loss: 0.3476 - val_accuracy: 0.8545\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3353 - accuracy: 0.8635 - val_loss: 0.3597 - val_accuracy: 0.8535\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3329 - accuracy: 0.8636 - val_loss: 0.3404 - val_accuracy: 0.8630\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3284 - accuracy: 0.8633 - val_loss: 0.3638 - val_accuracy: 0.8550\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3270 - accuracy: 0.8634 - val_loss: 0.3528 - val_accuracy: 0.8525\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3229 - accuracy: 0.8655 - val_loss: 0.3453 - val_accuracy: 0.8645\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3183 - accuracy: 0.8685 - val_loss: 0.3731 - val_accuracy: 0.8550\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3188 - accuracy: 0.8681 - val_loss: 0.3402 - val_accuracy: 0.8600\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3170 - accuracy: 0.8652 - val_loss: 0.3578 - val_accuracy: 0.8590\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3133 - accuracy: 0.8696 - val_loss: 0.3626 - val_accuracy: 0.8555\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3118 - accuracy: 0.8704 - val_loss: 0.3489 - val_accuracy: 0.8565\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3072 - accuracy: 0.8723 - val_loss: 0.3611 - val_accuracy: 0.8625\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3084 - accuracy: 0.8694 - val_loss: 0.3669 - val_accuracy: 0.8585\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3020 - accuracy: 0.8729 - val_loss: 0.3683 - val_accuracy: 0.8645\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3028 - accuracy: 0.8711 - val_loss: 0.3667 - val_accuracy: 0.8600\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,y_train,validation_data = (X_test, y_test), epochs = 100,\n",
    "    callbacks = [tensorflow_callback,early_stopping_callback]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Understanding `model.fit()` - Training Your Neural Network\n",
    "\n",
    "## What's Happening\n",
    "\n",
    "# 1. **Starting the Training Process**: `model.fit()` begins the training of your neural network.\n",
    "\n",
    "# 2. **Feeding Data**:\n",
    "#    - `X_train`: Your input features (customer data like age, credit score, etc.)\n",
    "#    - `y_train`: Your target values (whether customers exited or not)\n",
    "#    - `validation_data=(X_test, y_test)`: Separate data to check how well the model generalizes\n",
    "\n",
    "# 3. **Setting Training Duration**:\n",
    "#    - `epochs=100`: The model will go through the entire dataset up to 100 times\n",
    "#    - Each \"epoch\" is one complete pass through all training data\n",
    "\n",
    "# 4. **Using Callbacks for Monitoring and Control**:\n",
    "#    - `tensorflow_callback`: Records training metrics for visualization in TensorBoard\n",
    "#    - `early_stopping_callback`: Watches for when training stops improving and ends early\n",
    "\n",
    "# 5. **Behind the Scenes**:\n",
    "#    - For each batch of data:\n",
    "#      - The model makes predictions\n",
    "#      - Compares predictions to actual values\n",
    "#      - Calculates the loss (error)\n",
    "#      - Updates weights and biases to reduce the error\n",
    "#    - After each epoch, it evaluates on the validation data\n",
    "#    - Callbacks monitor progress and can take actions (like stopping training)\n",
    "\n",
    "# 6. **Storing Results**:\n",
    "#    - `history`: Captures training metrics over time (loss, accuracy for both training and validation)\n",
    "#    - This can be used later to plot learning curves\n",
    "\n",
    "# ## What You See in the Output\n",
    "\n",
    "# The output shows progress for each epoch:\n",
    "\n",
    "# ```language=\n",
    "# Epoch 1/100\n",
    "# 250/250 [==============================] - 1s 3ms/step - loss: 0.3435 - accuracy: 0.8590 - val_loss: 0.3397 - val_accuracy: 0.8590\n",
    "# ```\n",
    "\n",
    "\n",
    "# - `250/250`: Processed 250 batches out of 250 total\n",
    "# - `loss: 0.3435`: Training loss (error) for this epoch\n",
    "# - `accuracy: 0.8590`: Training accuracy (85.9% correct predictions)\n",
    "# - `val_loss: 0.3397`: Validation loss\n",
    "# - `val_accuracy: 0.8590`: Validation accuracy\n",
    "\n",
    "# The training stopped after 16 epochs (instead of the maximum 100) because the early stopping callback detected that the validation loss wasn't improving anymore, preventing overfitting and saving computation time.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Understanding Epochs and Batches in Neural Network Training\n",
    "\n",
    "## What Are Epochs?\n",
    "\n",
    "# An **epoch** means training your model on the entire dataset exactly once. \n",
    "\n",
    "# **Simple explanation:**\n",
    "# - Imagine you have a textbook with 1000 pages\n",
    "# - Reading the entire book once = 1 epoch\n",
    "# - Reading it again from start to finish = another epoch\n",
    "\n",
    "# In your code, `epochs=100` means:\n",
    "# - The model will process all your training data up to 100 times\n",
    "# - Each time through the data, the model learns and adjusts its weights\n",
    "# - It's like giving your model multiple chances to study the same material\n",
    "\n",
    "# ## Why 100 Epochs?\n",
    "\n",
    "# The number 100 is just a maximum limit, not a requirement. It means:\n",
    "# - \"Keep training for up to 100 complete passes through the data\"\n",
    "# - The early stopping callback will likely stop training before reaching 100\n",
    "\n",
    "# **How to choose the number:**\n",
    "# - Too few epochs: The model might not learn enough (underfitting)\n",
    "# - Too many epochs: The model might memorize the training data (overfitting)\n",
    "# - The \"right\" number depends on your specific data and model\n",
    "\n",
    "# **Common approaches:**\n",
    "# 1. Set a high number (like 100) but use early stopping to prevent overfitting\n",
    "# 2. Try different values and see which gives the best validation performance\n",
    "# 3. Look at learning curves to see when improvement plateaus\n",
    "\n",
    "# ## What Are Batches?\n",
    "\n",
    "# **Batches** are smaller chunks of your training data processed at one time.\n",
    "\n",
    "# **Simple explanation:**\n",
    "# - Instead of studying the entire textbook at once, you study it chapter by chapter\n",
    "# - Each \"chapter\" is a batch of data\n",
    "\n",
    "# In your output, `250/250` means:\n",
    "# - Your training data was divided into 250 batches\n",
    "# - The model has processed all 250 batches (completing one epoch)\n",
    "\n",
    "# ## Why Use Batches?\n",
    "\n",
    "# 1. **Memory efficiency**: Processing all data at once might not fit in memory\n",
    "# 2. **Training speed**: Updates happen more frequently, potentially speeding up learning\n",
    "# 3. **Better generalization**: Small random variations between batches can help the model generalize better\n",
    "\n",
    "# ## How Batch Size Is Determined\n",
    "\n",
    "# The default batch size in Keras is 32, meaning:\n",
    "# - If you have 8000 training examples, you'd have 8000 ÷ 32 = 250 batches\n",
    "\n",
    "# You can explicitly set the batch size with the `batch_size` parameter:\n",
    "# ```python\n",
    "# model.fit(X_train, y_train, batch_size=64, epochs=100, ...)\n",
    "# ```\n",
    "\n",
    "# **Choosing batch size:**\n",
    "# - Smaller batches (8-64): More updates, potentially better generalization, but slower training\n",
    "# - Larger batches (128-512): Faster training, but might generalize less well\n",
    "# - Very large batches: May require more epochs to reach the same performance\n",
    "\n",
    "# The optimal values for both epochs and batch size depend on your specific dataset and model architecture, and often require experimentation to find the best balance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/murli/murli/git_projects/employee_churn_modeling/myenv/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model.h5\")\n",
    "\n",
    "\n",
    "# Understanding .h5 vs .pkl File Formats\n",
    "\n",
    "## .h5 (HDF5) Format\n",
    "\n",
    "# **What it is:**\n",
    "# - HDF5 (Hierarchical Data Format version 5) is a file format designed for storing large, complex datasets\n",
    "# - It's optimized for storing numerical data arrays and supports complex hierarchical structures\n",
    "# - In deep learning, it's commonly used to save entire neural network models including architecture and weights\n",
    "\n",
    "# **Key features:**\n",
    "# - Efficient storage of large numerical arrays\n",
    "# - Hierarchical organization (like folders within folders)\n",
    "# - Fast read/write operations\n",
    "# - Cross-platform compatibility\n",
    "# - Supports partial reading (you can load just part of a file)\n",
    "\n",
    "# **When to use .h5:**\n",
    "# - When saving complete neural network models (architecture + weights)\n",
    "# - When working with large datasets that need efficient storage\n",
    "# - When you need to preserve the exact structure of complex data\n",
    "# - When using frameworks like TensorFlow/Keras that natively support it\n",
    "\n",
    "# ## .pkl (Pickle) Format\n",
    "\n",
    "# **What it is:**\n",
    "# - Pickle is Python's native serialization format\n",
    "# - It converts Python objects into byte streams that can be saved to disk and loaded later\n",
    "# - It can store almost any Python object (not just numerical data)\n",
    "\n",
    "# **Key features:**\n",
    "# - Python-specific (not easily used by other programming languages)\n",
    "# - Can serialize almost any Python object\n",
    "# - Simple to use with Python's built-in pickle module\n",
    "# - Less efficient for large numerical data compared to HDF5\n",
    "\n",
    "# **When to use .pkl:**\n",
    "# - When saving scikit-learn models or preprocessors (like your LabelEncoder)\n",
    "# - When storing Python-specific objects that aren't just numerical data\n",
    "# - When you need to quickly save and load Python objects without worrying about their internal structure\n",
    "# - When working with smaller datasets or models\n",
    "\n",
    "# ## Key Differences\n",
    "\n",
    "# 1. **Compatibility:**\n",
    "#    - .h5: Cross-platform, can be used by multiple programming languages\n",
    "#    - .pkl: Python-specific, not easily used outside Python\n",
    "\n",
    "# 2. **Performance:**\n",
    "#    - .h5: More efficient for large numerical datasets\n",
    "#    - .pkl: Less efficient for large data, but simpler for small objects\n",
    "\n",
    "# 3. **Use cases:**\n",
    "#    - .h5: Better for neural networks and large numerical datasets\n",
    "#    - .pkl: Better for scikit-learn models and general Python objects\n",
    "\n",
    "# 4. **Structure:**\n",
    "#    - .h5: Maintains hierarchical structure, allows partial access\n",
    "#    - .pkl: Serializes the entire object at once\n",
    "\n",
    "# In your notebook, you're using both formats appropriately:\n",
    "# - `.h5` for saving your neural network model\n",
    "# - `.pkl` would be appropriate for saving preprocessors like your encoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: kill: (113682) - No such process\n"
     ]
    }
   ],
   "source": [
    "!kill 113682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 113682), started 0:07:15 ago. (Use '!kill 113682' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-28d02c0a542af389\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-28d02c0a542af389\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
